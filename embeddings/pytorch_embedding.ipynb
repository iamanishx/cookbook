{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('gutenberg')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6mAehxVKLl14",
        "outputId": "d63d3a5c-4331-44c9-cf78-b2ab66059067"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "13e5cc57",
        "outputId": "946fee6b-ddfc-4c0b-9021-f29a1b5d3c7c"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "011f3ce8",
        "outputId": "09a577a5-6882-46da-98c0-f7c7fdfa2b3c"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from nltk.corpus import gutenberg\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "import re\n",
        "\n",
        "# 1. Load & Preprocess Data\n",
        "print(\"Loading and preprocessing text...\")\n",
        "\n",
        "alice = gutenberg.raw('carroll-alice.txt')\n",
        "\n",
        "sentences = []\n",
        "for sentence in sent_tokenize(alice):\n",
        "    cleaned_sentence = re.sub(r'[^a-zA-Z\\s]', '', sentence.lower())\n",
        "    words = word_tokenize(cleaned_sentence)\n",
        "    if words:\n",
        "        sentences.append(words)\n",
        "\n",
        "# Flatten into token list\n",
        "tokens = [w for sent in sentences for w in sent]\n",
        "\n",
        "# 2. Build Vocabulary\n",
        "vocab = list(set(tokens))\n",
        "word_to_ix = {word: i for i, word in enumerate(vocab)}\n",
        "ix_to_word = {i: word for word, i in word_to_ix.items()}\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "print(f\"Total tokens: {len(tokens)}\")\n",
        "print(f\"Vocabulary size: {vocab_size}\")\n",
        "\n",
        "# 3. Create Training Data (CBOW)\n",
        "def make_context_target(tokens, window_size=2):\n",
        "    data = []\n",
        "    for i in range(window_size, len(tokens) - window_size):\n",
        "        context = tokens[i - window_size:i] + tokens[i+1:i+window_size+1]\n",
        "        target = tokens[i]\n",
        "        data.append((context, target))\n",
        "    return data\n",
        "\n",
        "window_size = 2\n",
        "data = make_context_target(tokens, window_size=window_size)\n",
        "\n",
        "# 4. CBOW Model\n",
        "class CBOW(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim):\n",
        "        super(CBOW, self).__init__()\n",
        "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.linear = nn.Linear(embedding_dim, vocab_size)\n",
        "\n",
        "    def forward(self, context_idxs):\n",
        "        embeds = self.embeddings(context_idxs)\n",
        "        mean_embeds = embeds.mean(dim=0).view(1, -1)\n",
        "        out = self.linear(mean_embeds)\n",
        "        return out\n",
        "#https://docs.pytorch.org/docs/stable/generated/torch.nn.Embedding.html\n",
        "#https://docs.pytorch.org/docs/stable/generated/torch.nn.Linear.html\n",
        "\n",
        "# 5. Training Setup\n",
        "embedding_dim = 100\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = CBOW(vocab_size, embedding_dim).to(device)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
        "#https://docs.pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html\n",
        "#https://docs.pytorch.org/docs/stable/generated/torch.optim.SGD.html\n",
        "\n",
        "print(\"Training on:\", device)\n",
        "\n",
        "# 6. Training Loop\n",
        "epochs = 100\n",
        "for epoch in range(epochs):\n",
        "    total_loss = 0\n",
        "    for context, target in data:\n",
        "        context_idxs = torch.tensor([word_to_ix[w] for w in context], dtype=torch.long).to(device)\n",
        "        target_idx = torch.tensor([word_to_ix[target]], dtype=torch.long).to(device)\n",
        "\n",
        "        scores = model(context_idxs)\n",
        "        loss = loss_fn(scores, target_idx)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss:.4f}\")\n",
        "\n",
        "# 7. Extract Word Vectors\n",
        "word_vectors = model.embeddings.weight.data.cpu()\n",
        "\n",
        "print(\"\\nVector for 'alice':\")\n",
        "print(word_vectors[word_to_ix[\"alice\"]][:10])  # show first 10 dims"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading and preprocessing text...\n",
            "Total tokens: 26383\n",
            "Vocabulary size: 2751\n",
            "Training on: cuda\n",
            "Epoch 1/100, Loss: 175599.8108\n",
            "Epoch 2/100, Loss: 151633.2676\n",
            "Epoch 3/100, Loss: 142667.9521\n",
            "Epoch 4/100, Loss: 136857.3290\n",
            "Epoch 5/100, Loss: 132352.6871\n",
            "Epoch 6/100, Loss: 128564.9292\n",
            "Epoch 7/100, Loss: 125237.9990\n",
            "Epoch 8/100, Loss: 122236.7932\n",
            "Epoch 9/100, Loss: 119480.5221\n",
            "Epoch 10/100, Loss: 116917.1232\n",
            "Epoch 11/100, Loss: 114511.2352\n",
            "Epoch 12/100, Loss: 112237.6852\n",
            "Epoch 13/100, Loss: 110077.7939\n",
            "Epoch 14/100, Loss: 108017.3120\n",
            "Epoch 15/100, Loss: 106045.1349\n",
            "Epoch 16/100, Loss: 104152.4164\n",
            "Epoch 17/100, Loss: 102331.9488\n",
            "Epoch 18/100, Loss: 100577.7251\n",
            "Epoch 19/100, Loss: 98884.6753\n",
            "Epoch 20/100, Loss: 97248.4737\n",
            "Epoch 21/100, Loss: 95665.4422\n",
            "Epoch 22/100, Loss: 94132.4125\n",
            "Epoch 23/100, Loss: 92646.6728\n",
            "Epoch 24/100, Loss: 91205.8836\n",
            "Epoch 25/100, Loss: 89808.0191\n",
            "Epoch 26/100, Loss: 88451.3510\n",
            "Epoch 27/100, Loss: 87134.3775\n",
            "Epoch 28/100, Loss: 85855.8143\n",
            "Epoch 29/100, Loss: 84614.5367\n",
            "Epoch 30/100, Loss: 83409.5534\n",
            "Epoch 31/100, Loss: 82239.9475\n",
            "Epoch 32/100, Loss: 81104.8434\n",
            "Epoch 33/100, Loss: 80003.3620\n",
            "Epoch 34/100, Loss: 78934.5905\n",
            "Epoch 35/100, Loss: 77897.5803\n",
            "Epoch 36/100, Loss: 76891.3269\n",
            "Epoch 37/100, Loss: 75914.7672\n",
            "Epoch 38/100, Loss: 74966.7980\n",
            "Epoch 39/100, Loss: 74046.2765\n",
            "Epoch 40/100, Loss: 73152.0536\n",
            "Epoch 41/100, Loss: 72282.9541\n",
            "Epoch 42/100, Loss: 71437.8210\n",
            "Epoch 43/100, Loss: 70615.5266\n",
            "Epoch 44/100, Loss: 69814.9515\n",
            "Epoch 45/100, Loss: 69035.0177\n",
            "Epoch 46/100, Loss: 68274.6928\n",
            "Epoch 47/100, Loss: 67532.9888\n",
            "Epoch 48/100, Loss: 66808.9591\n",
            "Epoch 49/100, Loss: 66101.7052\n",
            "Epoch 50/100, Loss: 65410.3826\n",
            "Epoch 51/100, Loss: 64734.1880\n",
            "Epoch 52/100, Loss: 64072.3660\n",
            "Epoch 53/100, Loss: 63424.2109\n",
            "Epoch 54/100, Loss: 62789.0534\n",
            "Epoch 55/100, Loss: 62166.2758\n",
            "Epoch 56/100, Loss: 61555.2950\n",
            "Epoch 57/100, Loss: 60955.5662\n",
            "Epoch 58/100, Loss: 60366.5779\n",
            "Epoch 59/100, Loss: 59787.8616\n",
            "Epoch 60/100, Loss: 59218.9714\n",
            "Epoch 61/100, Loss: 58659.4924\n",
            "Epoch 62/100, Loss: 58109.0445\n",
            "Epoch 63/100, Loss: 57567.2608\n",
            "Epoch 64/100, Loss: 57033.8128\n",
            "Epoch 65/100, Loss: 56508.3827\n",
            "Epoch 66/100, Loss: 55990.6765\n",
            "Epoch 67/100, Loss: 55480.4218\n",
            "Epoch 68/100, Loss: 54977.3635\n",
            "Epoch 69/100, Loss: 54481.2544\n",
            "Epoch 70/100, Loss: 53991.8758\n",
            "Epoch 71/100, Loss: 53509.0129\n",
            "Epoch 72/100, Loss: 53032.4696\n",
            "Epoch 73/100, Loss: 52562.0557\n",
            "Epoch 74/100, Loss: 52097.5923\n",
            "Epoch 75/100, Loss: 51638.9168\n",
            "Epoch 76/100, Loss: 51185.8766\n",
            "Epoch 77/100, Loss: 50738.3128\n",
            "Epoch 78/100, Loss: 50296.0955\n",
            "Epoch 79/100, Loss: 49859.0884\n",
            "Epoch 80/100, Loss: 49427.1680\n",
            "Epoch 81/100, Loss: 49000.2158\n",
            "Epoch 82/100, Loss: 48578.1191\n",
            "Epoch 83/100, Loss: 48160.7775\n",
            "Epoch 84/100, Loss: 47748.0854\n",
            "Epoch 85/100, Loss: 47339.9531\n",
            "Epoch 86/100, Loss: 46936.2901\n",
            "Epoch 87/100, Loss: 46537.0071\n",
            "Epoch 88/100, Loss: 46142.0304\n",
            "Epoch 89/100, Loss: 45751.2763\n",
            "Epoch 90/100, Loss: 45364.6779\n",
            "Epoch 91/100, Loss: 44982.1648\n",
            "Epoch 92/100, Loss: 44603.6713\n",
            "Epoch 93/100, Loss: 44229.1389\n",
            "Epoch 94/100, Loss: 43858.5082\n",
            "Epoch 95/100, Loss: 43491.7222\n",
            "Epoch 96/100, Loss: 43128.7253\n",
            "Epoch 97/100, Loss: 42769.4732\n",
            "Epoch 98/100, Loss: 42413.9152\n",
            "Epoch 99/100, Loss: 42062.0107\n",
            "Epoch 100/100, Loss: 41713.7099\n",
            "\n",
            "Vector for 'alice':\n",
            "tensor([ 3.4095,  2.7741,  2.9807,  0.7712, -1.2017,  0.0052, -0.7414, -0.6551,\n",
            "        -0.3953, -0.7627])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def word_similarity(w1, w2):\n",
        "    v1 = word_vectors[word_to_ix[w1]]\n",
        "    v2 = word_vectors[word_to_ix[w2]]\n",
        "    sim = F.cosine_similarity(v1.unsqueeze(0), v2.unsqueeze(0))\n",
        "    return sim.item()\n",
        "\n",
        "print(\"Similarity(alice, wonderland):\", word_similarity(\"alice\", \"wonderland\"))\n",
        "print(\"Similarity(king, queen):\", word_similarity(\"king\", \"queen\") if \"king\" in word_to_ix else \"N/A\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oupr-sY0NpC4",
        "outputId": "4375abcc-e8ba-4eb0-ec16-c4067fbe40c5"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Similarity(alice, wonderland): 0.008554024621844292\n",
            "Similarity(king, queen): 0.20389409363269806\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QchzNI8rPQR6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "96b90a96",
        "outputId": "c6e4bfe0-d331-40fa-fb3e-9cc36804e205"
      },
      "source": [
        "model_save_path = \"cbow_model.pth\"\n",
        "torch.save(model.state_dict(), model_save_path)\n",
        "print(f\"Model saved to {model_save_path}\")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved to cbow_model.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for name, param in model.named_parameters():\n",
        "    print(name, param.shape, param.requires_grad)\n",
        "\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(\"Total parameters:\", total_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ULVNdWemaNDl",
        "outputId": "8f03d327-a3c4-4810-c8b8-0ae11624df52"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "embeddings.weight torch.Size([2751, 100]) True\n",
            "linear.weight torch.Size([2751, 100]) True\n",
            "linear.bias torch.Size([2751]) True\n",
            "Total parameters: 552951\n"
          ]
        }
      ]
    }
  ]
}