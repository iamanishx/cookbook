{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "156716ff",
      "metadata": {
        "id": "156716ff"
      },
      "source": [
        "# RNN, LSTM, GRU (+ Bi-variants) with Attention & FC -A PyTorch notebook\n",
        "  \n",
        "This notebook collects the material we discussed: what recurrence is, `batch/seq_len/input_size`, cell state in LSTM, why you need an `fc` head, attention (additive/Bahdanau style), and runnable PyTorch code for GRU/LSTM/RNN with attention as well as BiLSTM/BiGRU versions.  \n",
        "The toy dataset is intentionally tiny to run fast and demonstrate shapes, training loop, and attention visualization.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "410bae7b",
      "metadata": {
        "id": "410bae7b"
      },
      "source": [
        "## What you'll find here (quick tour)\n",
        "\n",
        "1. Quick conceptual recap (recurrence, embeddings, RNN outputs vs predictions).  \n",
        "2. A tiny synthetic dataset (token sequences) so we can show end-to-end code without downloading anything.  \n",
        "3. An **Attention** module (additive / Bahdanau-style).  \n",
        "4. Model classes: `SimpleRNN`, `LSTMModel`, `GRUModel`, `BiLSTMModel`, `BiGRUModel` — each with an `fc` head and optional attention.  \n",
        "5. Training & evaluation helpers (includes F1 score).  \n",
        "6. A small training run on the toy data and attention visualization on an example.  \n",
        "7. Notes: when to use each architecture, gotchas, and practical tips.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c02de9fb",
      "metadata": {
        "id": "c02de9fb"
      },
      "source": [
        "---\n",
        "## Recap\n",
        "\n",
        "- **Recurrence**: the network applies the same cell across time steps and feeds the previous hidden state into the next step (the loop). That's what RNN/LSTM/GRU do.  \n",
        "- **Embeddings**: `nn.Embedding` converts token ids → dense vectors. These are *representations*, not final answers.  \n",
        "- **RNN outputs**: RNN layers produce **hidden states** (representations). They **do not** directly provide class logits — that's the job of an output head (typically a fully connected `nn.Linear`, commonly named `fc`).  \n",
        "- **LSTM cell state**: `C_t` is the long-term memory conveyor belt; `h_t` is the short-term/output. Gates (forget/input/output) control updates.  \n",
        "- **Attention**: a learned weighting over time steps (or elements) telling the model *which parts of the sequence to focus on* when producing a final representation. Attention + RNN often improves interpretability and results on noisy/long sequences.\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "82de361b",
      "metadata": {
        "id": "82de361b"
      },
      "outputs": [],
      "source": [
        "# Minimal imports used by the notebook\n",
        "import math, random, os, sys, itertools, time\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Try sklearn for metrics; fall back to simple implementation if absent\n",
        "try:\n",
        "    from sklearn.metrics import f1_score\n",
        "    _HAS_SK = True\n",
        "except Exception:\n",
        "    _HAS_SK = False\n",
        "    def f1_score(y_true, y_pred, average='macro'):\n",
        "        # simple implementation for binary/multiclass macro f1\n",
        "        from collections import Counter\n",
        "        labels = sorted(set(y_true) | set(y_pred))\n",
        "        f1s = []\n",
        "        for lab in labels:\n",
        "            tp = sum(1 for yt, yp in zip(y_true, y_pred) if yt==lab and yp==lab)\n",
        "            fp = sum(1 for yt, yp in zip(y_true, y_pred) if yt!=lab and yp==lab)\n",
        "            fn = sum(1 for yt, yp in zip(y_true, y_pred) if yt==lab and yp!=lab)\n",
        "            prec = tp / (tp+fp) if (tp+fp)>0 else 0.0\n",
        "            rec = tp / (tp+fn) if (tp+fn)>0 else 0.0\n",
        "            f1s.append(2*prec*rec/(prec+rec) if (prec+rec)>0 else 0.0)\n",
        "        if average=='macro':\n",
        "            return sum(f1s)/len(f1s)\n",
        "        elif average=='micro':\n",
        "            # micro: compute global TP/FP/FN\n",
        "            tp = sum(1 for yt, yp in zip(y_true, y_pred) if yt==yp)\n",
        "            return tp/len(y_true)\n",
        "        else:\n",
        "            return sum(f1s)/len(f1s)\n",
        "print('torch:', torch.__version__, 'sklearn available?', _HAS_SK)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3706bb5d",
      "metadata": {
        "id": "3706bb5d"
      },
      "source": [
        "## Tiny synthetic dataset (text-like sequences)\n",
        "\n",
        "We'll make sequences of token ids from a small vocab. The task is intentionally simple so the focus is on the models and flows:\n",
        "- Each sequence length = `seq_len`\n",
        "- Vocabulary size small (e.g. 20)\n",
        "- Label is 1 if the special token `magic_token` appears, else 0 (binary classification)\n",
        "This produces a problem where attention is useful (it can highlight the time steps containing the magic token).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50854457",
      "metadata": {
        "id": "50854457"
      },
      "outputs": [],
      "source": [
        "class TinySeqDataset(Dataset):\n",
        "    def __init__(self, n_samples=2000, seq_len=20, vocab=20, magic_token=7, p_magic=0.2):\n",
        "        self.samples = []\n",
        "        rng = random.Random(42)\n",
        "        for _ in range(n_samples):\n",
        "            seq = [rng.randrange(1, vocab) for _ in range(seq_len)]\n",
        "            if rng.random() < p_magic:\n",
        "                pos = rng.randrange(0, seq_len)\n",
        "                seq[pos] = magic_token\n",
        "                label = 1\n",
        "            else:\n",
        "                label = 0\n",
        "            self.samples.append((torch.tensor(seq, dtype=torch.long), label))\n",
        "    def __len__(self): return len(self.samples)\n",
        "    def __getitem__(self, idx): return self.samples[idx]\n",
        "\n",
        "def collate(batch):\n",
        "    xs, ys = zip(*batch)\n",
        "    xs = torch.stack(xs, dim=0)\n",
        "    ys = torch.tensor(ys, dtype=torch.long)\n",
        "    return xs, ys\n",
        "\n",
        "# quick dataset\n",
        "train_ds = TinySeqDataset(n_samples=800, seq_len=20, vocab=20, magic_token=7, p_magic=0.35)\n",
        "val_ds   = TinySeqDataset(n_samples=200, seq_len=20, vocab=20, magic_token=7, p_magic=0.35)\n",
        "test_ds  = TinySeqDataset(n_samples=200, seq_len=20, vocab=20, magic_token=7, p_magic=0.35)\n",
        "\n",
        "train_dl = DataLoader(train_ds, batch_size=64, shuffle=True, collate_fn=collate)\n",
        "val_dl   = DataLoader(val_ds,   batch_size=128, shuffle=False, collate_fn=collate)\n",
        "test_dl  = DataLoader(test_ds,  batch_size=128, shuffle=False, collate_fn=collate)\n",
        "\n",
        "print('dataset sizes:', len(train_ds), len(val_ds), len(test_ds))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1a637ed3",
      "metadata": {
        "id": "1a637ed3"
      },
      "source": [
        "## Additive (Bahdanau-style) Attention module\n",
        "\n",
        "This will take RNN outputs across time `(batch, seq_len, hidden)` and produce:\n",
        "- a context vector `(batch, hidden)` — weighted sum of time-step hidden states\n",
        "- attention weights `(batch, seq_len)` for interpretation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dda18c7a",
      "metadata": {
        "id": "dda18c7a"
      },
      "outputs": [],
      "source": [
        "class AdditiveAttention(nn.Module):\n",
        "    def __init__(self, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.project = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.v = nn.Linear(hidden_dim, 1, bias=False)\n",
        "    def forward(self, h):  # h: (batch, seq_len, hidden)\n",
        "        # energy: (batch, seq_len, hidden)\n",
        "        energy = torch.tanh(self.project(h))\n",
        "        # scores: (batch, seq_len, 1)\n",
        "        scores = self.v(energy)\n",
        "        # weights: (batch, seq_len)\n",
        "        weights = F.softmax(scores.squeeze(-1), dim=1)\n",
        "        # context: (batch, hidden)\n",
        "        context = torch.sum(h * weights.unsqueeze(-1), dim=1)\n",
        "        return context, weights\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f9d1c05b",
      "metadata": {
        "id": "f9d1c05b"
      },
      "source": [
        "## Models: RNN / LSTM / GRU (+ bi- variants) with attention + FC head\n",
        "\n",
        "Each model:\n",
        "- Embedding layer (`nn.Embedding`): tokens → vectors\n",
        "- Recurrent layer: `nn.RNN`, `nn.LSTM`, or `nn.GRU` (batch_first=True)\n",
        "- Optional attention (we'll use attention for all RNN variants here to show the flow)\n",
        "- Fully-connected `fc` layer producing logits for binary classification\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65acbf3d",
      "metadata": {
        "id": "65acbf3d"
      },
      "outputs": [],
      "source": [
        "class BaseRNNClassifier(nn.Module):\n",
        "    def __init__(self, rnn_type='gru', vocab=20, emb_dim=32, hidden=64, num_layers=1, bidir=False, use_attention=True, num_classes=2):\n",
        "        super().__init__()\n",
        "        self.emb = nn.Embedding(vocab, emb_dim, padding_idx=0)\n",
        "        self.use_attention = use_attention\n",
        "        self.hidden = hidden\n",
        "        self.bidir = bidir\n",
        "        if rnn_type.lower() == 'gru':\n",
        "            self.rnn = nn.GRU(emb_dim, hidden, num_layers=num_layers, batch_first=True, bidirectional=bidir)\n",
        "        elif rnn_type.lower() == 'lstm':\n",
        "            self.rnn = nn.LSTM(emb_dim, hidden, num_layers=num_layers, batch_first=True, bidirectional=bidir)\n",
        "        elif rnn_type.lower() == 'rnn':\n",
        "            self.rnn = nn.RNN(emb_dim, hidden, num_layers=num_layers, batch_first=True, bidirectional=bidir)\n",
        "        else:\n",
        "            raise ValueError('unknown rnn type')\n",
        "        rnn_output_dim = hidden * (2 if bidir else 1)\n",
        "        if use_attention:\n",
        "            self.attn = AdditiveAttention(rnn_output_dim)\n",
        "            self.fc = nn.Linear(rnn_output_dim, num_classes)\n",
        "        else:\n",
        "            # if no attention, we'll use last timestep hidden state(s) -> fc\n",
        "            self.fc = nn.Linear(rnn_output_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch, seq_len)\n",
        "        emb = self.emb(x)  # (batch, seq_len, emb_dim)\n",
        "        rnn_out, hidden = self.rnn(emb)  # rnn_out: (batch, seq_len, hidden * directions)\n",
        "        if self.use_attention:\n",
        "            context, weights = self.attn(rnn_out)  # (batch, hidden*dir), (batch, seq_len)\n",
        "            logits = self.fc(context)\n",
        "            return logits, weights\n",
        "        else:\n",
        "            # use last valid time step\n",
        "            # for bidirectional and multiple layers we take last layer outputs\n",
        "            if isinstance(hidden, tuple):  # LSTM -> hidden is (h_n, c_n)\n",
        "                h_n = hidden[0]\n",
        "            else:\n",
        "                h_n = hidden\n",
        "            # h_n: (num_layers * directions, batch, hidden)\n",
        "            # take last layer's forward & backward\n",
        "            if self.bidir:\n",
        "                # concat forward and backward of last layer\n",
        "                last = torch.cat([h_n[-2], h_n[-1]], dim=1)\n",
        "            else:\n",
        "                last = h_n[-1]\n",
        "            logits = self.fc(last)\n",
        "            # produce dummy weights\n",
        "            return logits, None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1937e47c",
      "metadata": {
        "id": "1937e47c"
      },
      "source": [
        "## Training & eval helpers (small, clear functions)\n",
        "\n",
        "We'll use a simple training loop and compute accuracy + macro F1 on validation data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "004a1770",
      "metadata": {
        "id": "004a1770"
      },
      "outputs": [],
      "source": [
        "def train_epoch(model, dl, optim, criterion, device):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    for x, y in dl:\n",
        "        x = x.to(device); y = y.to(device)\n",
        "        optim.zero_grad()\n",
        "        logits, _ = model(x)\n",
        "        loss = criterion(logits, y)\n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "        total_loss += loss.item() * x.size(0)\n",
        "    return total_loss / len(dl.dataset)\n",
        "\n",
        "def evaluate(model, dl, device):\n",
        "    model.eval()\n",
        "    ys, ypreds = [], []\n",
        "    with torch.no_grad():\n",
        "        for x, y in dl:\n",
        "            x = x.to(device)\n",
        "            logits, _ = model(x)\n",
        "            preds = torch.argmax(logits, dim=1).cpu().tolist()\n",
        "            ys.extend(y.tolist())\n",
        "            ypreds.extend(preds)\n",
        "    acc = sum(1 for a,b in zip(ys, ypreds) if a==b) / len(ys)\n",
        "    f1 = f1_score(ys, ypreds, average='macro')\n",
        "    return acc, f1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "69ee567e",
      "metadata": {
        "id": "69ee567e"
      },
      "source": [
        "## Quick training run (GRU + Attention) — tiny run to demonstrate flow\n",
        "\n",
        "We train for a few epochs on the tiny synthetic dataset so you can see loss dropping and get attention visualizations. This is just a demonstration; real-world runs need more data and tuning.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b12d447a",
      "metadata": {
        "id": "b12d447a"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = BaseRNNClassifier(rnn_type='gru', vocab=20, emb_dim=32, hidden=64, bidir=False, use_attention=True, num_classes=2).to(device)\n",
        "optim = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "for epoch in range(1, 6):\n",
        "    t0 = time.time()\n",
        "    train_loss = train_epoch(model, train_dl, optim, criterion, device)\n",
        "    val_acc, val_f1 = evaluate(model, val_dl, device)\n",
        "    print(f'Epoch {epoch} train_loss={train_loss:.4f} val_acc={val_acc:.4f} val_f1={val_f1:.4f} time={(time.time()-t0):.2f}s')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "79e47b14",
      "metadata": {
        "id": "79e47b14"
      },
      "source": [
        "## Visualizing attention weights for a sample\n",
        "\n",
        "We pick one sample from the test set, run the model, and plot attention weights across the sequence. This shows what the attention mechanism focused on.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94d7701d",
      "metadata": {
        "id": "94d7701d"
      },
      "outputs": [],
      "source": [
        "# get a batch and visualize the first sample\n",
        "model.eval()\n",
        "x_batch, y_batch = next(iter(test_dl))\n",
        "x0 = x_batch[0:1].to(device)\n",
        "logits, weights = model(x0)\n",
        "pred = logits.argmax(dim=1).item()\n",
        "weights = weights[0].cpu().numpy()  # (seq_len,)\n",
        "seq = x0[0].cpu().numpy()\n",
        "\n",
        "print('true label:', y_batch[0].item(), 'pred:', pred)\n",
        "plt.figure(figsize=(8,2))\n",
        "plt.plot(weights, marker='o')\n",
        "plt.title('Attention weights across time (higher = more attended)')\n",
        "plt.xlabel('time step')\n",
        "plt.ylabel('attention weight')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# print sequence with weights for inspection\n",
        "print('tokens:', seq.tolist())\n",
        "print('attn  :', [float(f'{w:.3f}') for w in weights.tolist()])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0abe08eb",
      "metadata": {
        "id": "0abe08eb"
      },
      "source": [
        "---\n",
        "## Notes, tips, and practical takeaways\n",
        "\n",
        "- **Why `fc`?** RNN/LSTM/GRU output vector representations. `fc` maps representation → task space (classes, regression values, vocab logits). Without it there's no supervision signal that matches labels.  \n",
        "- **Attention is cheap and interpretable** for many sequence classification tasks. Use additive (Bahdanau) attention for simplicity. Dot-product attention is slightly faster when dims align.  \n",
        "- **Bi-directional RNNs (BiLSTM / BiGRU)** read sequence forward and backward and concatenate outputs. They give richer per-step features but require access to full sequence (not streaming).  \n",
        "- **GRU vs LSTM vs RNN**: GRU is a lighter LSTM (fewer gates), often a good default. LSTM has separate cell state for longer memory. Vanilla RNNs rarely match LSTM/GRU on long sequences due to vanishing gradients.  \n",
        "- **MultiHeadAttention**: usually better used when you remove recurrence (Transformers) or when you want dense pairwise interactions; it's quadratic in time and often redundant when used with recurrent layers.  \n",
        "- **Metrics**: prefer macro-F1 for imbalanced classes. Accuracy is misleading for skewed data.  \n",
        "- **Next steps**: add masking for variable-length sequences, compare BiGRU/BiLSTM, add dropout, tune hidden sizes, train on a real dataset (UCI HAR or an NLP dataset) and visualize attention across classes.\n",
        "---\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}